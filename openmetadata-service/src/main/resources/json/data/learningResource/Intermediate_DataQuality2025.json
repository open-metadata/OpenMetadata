{
  "name": "Intermediate_DataQuality2025",
  "displayName": "Data Quality: Dimensions, Testing & Monitoring Best Practices",
  "description": "Comprehensive guide to data quality covering six core dimensions, testing strategies, monitoring approaches, and best practices for maintaining high-quality data in modern data platforms.",
  "resourceType": "Article",
  "categories": [
    "DataQuality",
    "DataGovernance"
  ],
  "difficulty": "Intermediate",
  "source": {
    "url": "https://www.getcollate.io/learning-center/resource/Intermediate_DataQuality2025",
    "provider": "Collate",
    "embedConfig": {
      "content": "# Data Quality: Dimensions, Testing & Monitoring Best Practices\n\nComprehensive guide to ensuring data quality across your organization.\n\n## What is Data Quality?\n\nData quality refers to the degree to which data meets the requirements for its intended use. High-quality data is accurate, complete, consistent, and timely.\n\n## Six Dimensions of Data Quality\n\n### 1. Accuracy\n- Data correctly represents real-world entities\n- Free from errors and distortions\n- Validated against authoritative sources\n- **Example**: Customer email addresses that are properly formatted and deliverable\n\n### 2. Completeness\n- All required data is present\n- No missing values in critical fields\n- Percentage of populated fields\n- **Example**: All mandatory customer profile fields are filled\n\n### 3. Consistency\n- Data is uniform across systems\n- No contradictions between related fields\n- Same values mean the same thing everywhere\n- **Example**: Customer status codes mean the same across all databases\n\n### 4. Timeliness\n- Data is up-to-date and available when needed\n- Reflects current state of reality\n- Updated according to SLAs\n- **Example**: Inventory counts updated in real-time or near real-time\n\n### 5. Validity\n- Data conforms to defined formats and rules\n- Meets business constraints\n- Within acceptable ranges\n- **Example**: Date fields contain valid dates, not future dates for birthdates\n\n### 6. Uniqueness\n- No unintended duplicates\n- Single source of truth for entities\n- Proper deduplication processes\n- **Example**: Each customer has exactly one record, not multiple duplicates\n\n## Data Quality Testing\n\n### Schema Validation Tests\n- Column names and types match expectations\n- Required columns are present\n- Data types are correct\n\n### Constraint Tests\n- NOT NULL constraints\n- UNIQUE constraints\n- FOREIGN KEY relationships\n- CHECK constraints for ranges\n\n### Business Rule Tests\n- Domain-specific validations\n- Cross-field dependencies\n- Calculated field accuracy\n- Referential integrity\n\n### Statistical Tests\n- Distribution analysis\n- Outlier detection\n- Variance monitoring\n- Trend analysis\n\n## Data Quality Monitoring\n\n### Proactive Monitoring\n- **Automated Tests**: Run tests on schedules (hourly, daily, weekly)\n- **Real-time Checks**: Validate data during ingestion\n- **Anomaly Detection**: ML models detect unusual patterns\n- **Alerts**: Notify stakeholders when quality degrades\n\n### Reactive Monitoring\n- **Incident Tracking**: Log and track quality issues\n- **Root Cause Analysis**: Investigate failures\n- **Remediation Workflows**: Fix issues systematically\n- **Post-mortems**: Learn from incidents\n\n### Metrics to Track\n- **Test Pass Rate**: % of tests passing over time\n- **Mean Time to Detection (MTTD)**: How quickly issues are found\n- **Mean Time to Resolution (MTTR)**: How quickly issues are fixed\n- **Data Quality Score**: Composite metric across dimensions\n- **Freshness**: Age of data since last update\n\n## Modern Data Quality Tools\n\n### Capabilities to Look For\n- **No-code test creation**: Business users can define tests\n- **Column-level lineage**: Track quality upstream and downstream\n- **Anomaly detection**: AI-powered quality checks\n- **Custom SQL tests**: Define complex business rules\n- **Integration with dbt**: Leverage dbt tests in your catalog\n- **Incident management**: Built-in workflows for issue resolution\n- **Quality dashboards**: Visualize quality trends\n\n### OpenMetadata Features\n- Native data quality framework\n- Integration with Great Expectations\n- Custom test definitions\n- Test suites and test cases\n- Quality incident tracking\n- Quality metrics on data assets\n\n## Best Practices\n\n### 1. Define Quality Standards\n- Document what \"good quality\" means for each dataset\n- Set thresholds and SLAs\n- Get stakeholder agreement\n\n### 2. Implement Progressive Testing\n- Start with critical datasets\n- Begin with simple tests (NOT NULL, uniqueness)\n- Gradually add complex business rules\n- Expand coverage over time\n\n### 3. Assign Ownership\n- Data owners define quality standards\n- Data engineers implement tests\n- Data stewards monitor and respond to issues\n- Clear escalation paths\n\n### 4. Test Early and Often\n- Test at ingestion time\n- Test during transformations\n- Test before publishing to consumers\n- Continuous testing mindset\n\n### 5. Communicate Quality\n- Make quality metrics visible to consumers\n- Show trends over time\n- Document known issues\n- Provide context for quality scores\n\n### 6. Automate Remediation\n- Auto-quarantine bad data\n- Trigger data repair workflows\n- Notify responsible parties\n- Track resolution status\n\n## Common Data Quality Issues\n\n### Upstream Problems\n- Source system changes breaking downstream\n- Schema drift without notification\n- Data type mismatches\n\n### Transformation Errors\n- Logic bugs in ETL/ELT code\n- Incorrect join conditions\n- Aggregation mistakes\n\n### Integration Issues\n- Duplicate records from multiple sources\n- Conflicting values for same entity\n- Missing referential integrity\n\n### Staleness\n- Pipelines failing silently\n- Delayed updates\n- Incomplete incremental loads\n\n## Success Metrics\n\n- **Coverage**: % of critical datasets with quality tests\n- **Pass Rate**: % of tests passing consistently\n- **Consumer Confidence**: User satisfaction with data reliability\n- **Incident Reduction**: Decrease in data-related issues over time\n- **Time Saved**: Reduction in time spent firefighting data issues\n\nFor more information, visit the [Collate Learning Center](https://www.getcollate.io/learning-center?topic=data-quality)"
    }
  },
  "estimatedDuration": 20,
  "contexts": [
    {
      "pageId": "glossary",
      "componentId": "glossary-header"
    },
    {
      "pageId": "glossaryTerm",
      "componentId": "glossary-term-header"
    }
  ],
  "status": "Active",
  "owners": []
}
