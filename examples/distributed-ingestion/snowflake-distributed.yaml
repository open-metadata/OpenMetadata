# Snowflake Distributed Ingestion Example
# This example shows how to ingest a Snowflake database with 10,000+ tables
# using distributed execution across 100 parallel pods.

source:
  type: snowflake
  serviceName: snowflake-prod
  serviceConnection:
    config:
      type: Snowflake
      username: ${SNOWFLAKE_USER}
      password: ${SNOWFLAKE_PASSWORD}
      account: ${SNOWFLAKE_ACCOUNT}
      warehouse: ${SNOWFLAKE_WAREHOUSE}
      database: ${SNOWFLAKE_DATABASE}
  sourceConfig:
    config:
      type: DatabaseMetadata
      # Optional: Filter to specific schemas
      # schemaFilterPattern:
      #   includes:
      #     - "^PUBLIC$"
      #     - "^ANALYTICS$"
      includeViews: true
      includeTables: true
      includeStoredProcedures: false
      markDeletedTables: true

sink:
  type: metadata-rest
  config:
    api_endpoint: null

workflowConfig:
  openMetadataServerConfig:
    hostPort: ${OPENMETADATA_HOST_PORT:-http://localhost:8585/api}
    authProvider: openmetadata
    securityConfig:
      jwtToken: ${OM_JWT_TOKEN}

  # Distributed Execution Configuration
  distributedExecution:
    enabled: true
    orchestrator: argo
    parallelism: 100  # Process 100 tables in parallel
    namespace: openmetadata
    image: openmetadata/ingestion:latest

    retryPolicy:
      maxAttempts: 3
      backoffSeconds: 60
      backoffFactor: 2
      maxBackoffSeconds: 600

    resourceRequests:
      cpu: "1"
      memory: "2Gi"

    resourceLimits:
      cpu: "2"
      memory: "4Gi"

    waitForCompletion: false  # Submit and return immediately
    timeoutSeconds: 86400  # 24 hours

  # Standard workflow config
  loggerLevel: INFO
  raiseOnError: false
  successThreshold: 90
